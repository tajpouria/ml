{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "What is Machine Learning?\n",
    "\n",
    "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "\n",
    "Supervised learning is a type of machine learning where the algorithm learns from labeled data to make predictions or decisions. In supervised learning, the model is trained on a set of input-output pairs, where the input is also known as the feature or predictor, and the output is the target or response variable. The goal of the algorithm is to learn a mapping between the input and output variables to make accurate predictions on new, unseen data.\n",
    "\n",
    "A common example of supervised learning is predicting house prices. In this case, the input features might include the number of bedrooms, the square footage of the house, the neighborhood, and other relevant information. The output variable is the price of the house. The algorithm is trained on a dataset of labeled examples, where each example contains the input features and the corresponding target price.\n",
    "\n",
    "Supervised learning is also used for other types of problems, such as classification and regression. In classification problems, the output variable is a categorical variable, such as a binary label (e.g., spam or not spam) or a multiclass label (e.g., cat, dog, or bird). In regression problems, the output variable is a continuous variable, such as predicting the price of a house or the stock price of a company.\n",
    "\n",
    "Regression problems are a common type of supervised learning problem, where the goal is to predict a numerical value or quantity. The input features are used to make a prediction about the target variable. Examples of regression problems include predicting the price of a house based on its features, predicting the weight of a person based on their height, and predicting the sales of a product based on its price.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between Regression and Classification\n",
    "\n",
    "Regression and classification are two important types of supervised learning problems in machine learning. Both involve the use of labeled data to make predictions or decisions, but they differ in the type of output variable they are trying to predict.\n",
    "\n",
    "Regression problems are those where the output variable is a continuous numerical value. The goal is to predict the value of a target variable based on input features. Examples of regression problems include predicting the price of a house based on its size, number of rooms, and location, or predicting the temperature of a city based on various weather features. The output of a regression algorithm is a range of values, rather than a discrete set of categories.\n",
    "\n",
    "On the other hand, classification problems involve predicting the categorical class or label of a given input based on the features of the input. The output of a classification algorithm is a categorical variable or a discrete set of categories. For example, a spam filter is a common example of a binary classification problem, where the goal is to predict whether an email is spam or not. In a multiclass classification problem, such as image classification, the algorithm must predict one of several possible categories for a given image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "Unsupervised learning is a type of machine learning where an algorithm is trained on a dataset without explicit supervision or labeled data. The goal of unsupervised learning is to uncover patterns, structures, and relationships within the data, without being told what to look for.\n",
    "\n",
    "One of the most common unsupervised learning algorithms is clustering. Clustering algorithms group similar data points together based on their features or attributes. For example, a clustering algorithm could be used to group customers with similar purchase behavior, or to group news articles with similar topics.\n",
    "\n",
    "Another example of unsupervised learning is dimensionality reduction. Dimensionality reduction algorithms reduce the number of features or attributes in a dataset, while still retaining as much information as possible. This can be useful for visualizing high-dimensional data, or for reducing the computational complexity of a machine learning model.\n",
    "\n",
    "One famous example of an unsupervised learning algorithm is the cocktail party algorithm. This algorithm is used to separate mixed audio signals into their component parts. Imagine being at a cocktail party where multiple people are talking at once. The cocktail party algorithm can be used to separate the different voices and identify what each person is saying, based on the different sound waves in the mixed audio signal.\n",
    "\n",
    "The cocktail party algorithm works by using independent component analysis (ICA), a technique that assumes that the observed signals are linear combinations of unknown source signals. By applying ICA, the cocktail party algorithm can estimate the sources and separate them from the mixed signal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "Linear regression is a statistical technique used to model the relationship between two variables, where one variable (called the dependent variable) is predicted from the other variable (called the independent variable). In other words, it helps to establish a linear relationship between the variables.\n",
    "\n",
    "One of the most common applications of linear regression is in the field of real estate, where it is used to predict the value of a property based on various factors such as the size of the house, location, number of bedrooms, and so on. In this article, we will explore how linear regression can be used to predict the housing price based on the size of the house.\n",
    "\n",
    "Let's consider a scenario where a real estate agent wants to estimate the selling price of a house based on its size. The agent has a dataset of previous house sales, which includes the size of the house in square feet and the corresponding sale price in dollars. The agent wants to use this data to build a linear regression model that can predict the sale price of a new house based on its size.\n",
    "\n",
    "To build the model, we first need to plot the data points on a scatter plot to visualize the relationship between the two variables. The scatter plot allows us to see if there is a linear relationship between the size of the house and the sale price.\n",
    "\n",
    "![Scatter Plot Analysis of Housing Prices and Sizes](attachment/scatter-plot-of-housing-price-vs-size.png)\n",
    "\n",
    "From the scatter plot, we can see that there is a positive linear relationship between the size of the house and the sale price. This means that as the size of the house increases, the sale price also tends to increase.\n",
    "\n",
    "Next, we need to find the equation of the line that best fits the data. This equation will be used to predict the sale price of a new house based on its size. The equation of the line is given by:\n",
    "\n",
    "y = mx + b\n",
    "\n",
    "where y is the sale price, x is the size of the house, m is the slope of the line, and b is the y-intercept.\n",
    "\n",
    "Using the dataset, we can calculate the slope and y-intercept of the line as:\n",
    "\n",
    "m = 223.178\n",
    "b = 101903.080\n",
    "\n",
    "Therefore, the equation of the line that best fits the data is:\n",
    "\n",
    "y = 223.178x + 101903.080\n",
    "\n",
    "This equation can be used to predict the sale price of a new house based on its size. For example, if the size of the house is 2000 square feet, the predicted sale price would be:\n",
    "\n",
    "y = 223.178 \\* 2000 + 101903.080 = $448,259.160"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "- Data Set: This refers to the entire collection of data points in the dataset, which includes both input and output variables. In this case, the data set may include information on the size of a house (input variable $x$) and its corresponding price (output variable $y$).\n",
    "\n",
    "- $x$: The input variable or feature, represents the independent variable or predictor variable in the dataset. In this case, the size of the house is the input variable $x$.\n",
    "\n",
    "- $y$: The output variable or target variable, represents the dependent variable or response variable in the dataset. In this case, the price of the house is the output variable $y$.\n",
    "\n",
    "- $m$: The number of training examples, refers to the total number of data points in the dataset. In this case, $m$ would represent the total number of houses for which we have both size and price information.\n",
    "\n",
    "- $(x, y)$: A single training example, represents a single data point in the dataset. In this case, a single training example would represent a single house, with its corresponding size and price information.\n",
    "\n",
    "- $(x(i), y(i))$: The i-th training example, refers to the i-th data point in the dataset, where i is an index ranging from 1 to $m$. In this case, $(x(i), y(i))$ would represent the size and price information for the i-th house in the dataset.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Function for Linear Regression\n",
    "\n",
    "The model function for linear regression is $f(x) = wx + b$, where $x$ is the input variable, $w$ is the weight or slope coefficient that determines the slope of the line, and $b$ is the intercept or bias term that determines where the line intersects the y-axis. The goal of linear regression is to find the best-fitting line that can predict the value of the output variable based on the input variable.\n",
    "\n",
    "The weight coefficient $(w)$ and the intercept term $(b)$ in linear regression are typically calculated using a statistical technique called \"ordinary least squares.\" This involves finding the line that minimizes the sum of the squared differences between the predicted values and the actual values of the output variable for the given input values. Once the line is fitted to the data, the weight coefficient and intercept term can be calculated using mathematical formulas.\n",
    "\n",
    "[C1_W1_Lab03_Model_Representation_Soln.ipynb](labs/C1-Supervised-Machine-Learning-Regression-and-Classification/week1/Optional-Labs/C1_W1_Lab03_Model_Representation_Soln.ipynb)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function: Mean Squared Error\n",
    "\n",
    "The squared error cost function, also known as the mean squared error (MSE) cost function, is a commonly used metric for evaluating the performance of regression models. It measures the average squared difference between the predicted and actual values.\n",
    "\n",
    "The formula for the squared error cost function is as follows:\n",
    "\n",
    "$$J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (y_i - \\hat{y_i})^2$$\n",
    "\n",
    "where $m$ is the number of training examples, $y_i$ is the actual value of the output variable for the i-th training example, and $\\hat{y_i}$ is the predicted value of the output variable for the i-th training example or $f(x_i)$.\n",
    "\n",
    "The cost function is a commonly used cost function in linear regression, where we are trying to find the optimal values of the weight vector $w$ and the bias term $b$ for a given set of input features $x$.\n",
    "\n",
    "In the equation $f(x)w,b = wx + b$, the weight vector $w$ represents the slopes of the linear regression model and the bias term $b$ represents the y-intercept. The goal of linear regression is to find the optimal values of w and b that **minimize** the cost function $J(w, b)$.\n",
    "\n",
    "The cost function $J(w, b)$ is a measure of the error between the predicted values $(ŷ_i)$ and the actual values $(y_i)$ for a set of training examples. The term $(y_i - ŷ_i)^2$ represents the squared difference between the actual and predicted values, and the summation over all training examples in the dataset accounts for the total error across the entire dataset.\n",
    "\n",
    "The factor $(1/2m)$ is a scaling factor that ensures that the cost function is normalized by the number of training examples $(m)$. By minimizing this cost function using optimization techniques like gradient descent, we can find the optimal values of w and b that minimize the overall error of the model.\n",
    "\n",
    "[C1_W1_Lab03_Model_Representation_Soln.ipynb](labs/C1-Supervised-Machine-Learning-Regression-and-Classification/week1/Optional-Labs/C1_W1_Lab03_Model_Representation_Soln.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
